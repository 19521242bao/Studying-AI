# -*- coding: utf-8 -*-
"""submission_ AI_TEMPO_RUN_EDA_enhance_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yv_d6zHyapmsF0F8hpL84M7SLrpCoyIO

# download data
"""

#train data
!gdown --id 1iNpj_H3OiJLydq5wF_r75At9wZuULZ9e

#public test data
!gdown --id 1Fuq6J2xHLVZ4BQnP22fxyPYHLh9Tvb4e

"""# import package"""

import pandas as pd 
import os
import numpy as np
import matplotlib.pyplot as plt
import warnings
import seaborn as sns 
from scipy import stats
from scipy.stats import norm
warnings.filterwarnings('ignore')
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn import metrics
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score,  auc, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,  GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

"""# demo data"""

data_train = pd.read_csv('/content/train.csv')
data_test = pd.read_csv('/content/public_test.csv')

data_train.head(13).T

data_train.info()

data_train.describe()

data_test.head(13).T

"""# filling missing value"""

def fill_miss_value(data):
  num_of_miss_data = data.isnull().sum()
  missing_data = pd.concat([num_of_miss_data], axis=1, keys = ['Number of missing value'])
  return missing_data
fill_miss_value(data_train)

#fill data age
data_train['age'] = data_train['age'].interpolate(method = 'pad', limit = 2)
data_train['age'] = data_train['age'].fillna(int(np.mean(data_train['age'])))

#fill data hypertension
data_train['hypertension'] = data_train['hypertension'].interpolate(method = 'pad', limit = 3)

#fill heart disease
data_train['heart_disease'] = data_train['heart_disease'].interpolate(method = 'pad', limit = 2)

#fill data glucose
data_train['avg_glucose_level'] = data_train['avg_glucose_level'].interpolate(method = 'pad', limit = 2)
data_train['avg_glucose_level'] = data_train['avg_glucose_level'].fillna(int(np.mean(data_train['avg_glucose_level'])))

#fill data bmi
data_train['bmi'] = data_train['bmi'].interpolate(method = 'pad', limit = 2)
data_train['bmi'] = data_train['bmi'].fillna(int(np.mean(data_train['bmi'])))

#fill data blood
data_train['blood'] = data_train['blood'].interpolate(method = 'pad', limit = 2)

#fill data height
data_train['height'] = data_train['height'].interpolate(method = 'pad', limit = 2)
data_train['height'] = data_train['height'].fillna(int(np.mean(data_train['height'])))

fill_miss_value(data_train)

data_train.head(13).T

data_train = data_train.drop(['id'], axis = 1)

"""#visualization data and see the distribution of feartures"""

#@title
#split data into numerical and categorical type
data_numerical=data_train[['age','avg_glucose_level','bmi']]
data_categorical=data_train[['gender', 'hypertension', 'heart_disease', 'ever_married','work_type', 'Residence_type', 
                       'smoking_status','blood', 'stroke']]
data_test['avg_glucose_level'].max()

"""Trực quan hóa và nhận xét:
1. Độ tuổi nằm trong khoảng 0-100, trong đó trong mức 50 chiến tỷ lệ nhiều nhất
2. Chỉ số IBM nằm trong khoảng 0-50, trong đó mức từ 20-30 chiến tỷ lệ nhiều nhất
3. Chỉ số glucos trung bình nằm trong khoảng từ 50 - 300 và chiế tỷ lệ nhiều nhất là khoảng từ 50 - 150
4. Độ lệnh phân phối cao

"""

#@title
#cre: https://www.kaggle.com/ruthvikpvs/stroke-data-eda-and-prediction
fig = plt.figure(figsize=(20,13))
gs = fig.add_gridspec(3,3)
gs.update(wspace=0.4, hspace=0.4)
# adding figures
ax0 = fig.add_subplot(gs[0,0])
ax1 = fig.add_subplot(gs[0,1])
ax2 = fig.add_subplot(gs[1,0])
ax3 = fig.add_subplot(gs[1,1])
ax4 = fig.add_subplot(gs[2,0])
ax5 = fig.add_subplot(gs[2,1])
axes=[ax0,ax1,ax2,ax3,ax4,ax5]
background_color = '#f6f5f7'
for i in axes:
    i.set_facecolor(background_color)
fig.patch.set_facecolor(background_color) 
#https://www.geeksforgeeks.org/kde-plot-visualization-with-pandas-and-seaborn/
sns.kdeplot(ax=ax0,x=data_train.loc[data_train['stroke']==1]['age'],color='crimson',label='Stroke',shade=True)
sns.kdeplot(ax=ax0,x=data_train.loc[data_train['stroke']==0]['age'],color='coral',label='No Stroke',shade=True)
ax0.legend(loc = 'upper left')
ax0.grid(linestyle='--', axis='y')

ax1.text(0.5,0.5,'Distribution of Age wrt Stroke',horizontalalignment = 'center',verticalalignment = 'center',fontsize = 18,fontfamily='serif')

sns.kdeplot(ax=ax2,x=data_train.loc[data_train['stroke']==1]['avg_glucose_level'],color='crimson',label='Stroke',shade=True)
sns.kdeplot(ax=ax2,x=data_train.loc[data_train['stroke']==0]['avg_glucose_level'],color='coral',label='No Stroke',shade=True)
ax2.legend(loc = 'upper right')
ax2.grid(linestyle='--', axis='y')

ax3.text(0.5,0.5,'Distribution of Glucose level\n wrt Stroke',horizontalalignment = 'center',verticalalignment = 'center',fontsize = 18,fontfamily='serif')


sns.kdeplot(ax=ax4,x=data_train.loc[data_train['stroke']==1]['bmi'],color='crimson',label='Stroke',shade=True)
sns.kdeplot(ax=ax4,x=data_train.loc[data_train['stroke']==0]['bmi'],color='coral',label='No Stroke',shade=True)
ax4.legend(loc = 'upper right')
ax4.grid(linestyle='--', axis='y')

ax5.text(0.5,0.5,'Distribution of BMI\n wrt Stroke',horizontalalignment = 'center',verticalalignment = 'center',fontsize = 18,fontfamily='serif')
# removing labels

axes1=[ax1,ax3,ax5]
for i in axes1:
    i.spines["bottom"].set_visible(False)
    i.spines["left"].set_visible(False)
    i.set_xlabel("")
    i.set_ylabel("")
    i.set_xticklabels([])
    i.set_yticklabels([])
    i.tick_params(left=False, bottom=False)
# removing spines of figures
for i in ["top","left","right"]:
    ax0.spines[i].set_visible(False)
    ax1.spines[i].set_visible(False)
    ax2.spines[i].set_visible(False)
    ax3.spines[i].set_visible(False)
    ax4.spines[i].set_visible(False)
    ax5.spines[i].set_visible(False)

#@title
fig = plt.figure(figsize=(17,12))
gs = fig.add_gridspec(2,2)
ax0=fig.add_subplot(gs[0,0])
ax1=fig.add_subplot(gs[0,1])
ax2=fig.add_subplot(gs[1,0])
axes=[ax0,ax1,ax2]
background_color = '#f6f5f7'
for i in axes:
    i.set_facecolor(background_color)
fig.patch.set_facecolor(background_color) 
# Age and bmi
sns.scatterplot(ax=ax0,x=data_numerical['age'],y=data_numerical['bmi'],hue=data_categorical['stroke'],palette="OrRd")
ax0.set_title('Age and BMI',fontweight='bold')
# Age and Glucose
sns.scatterplot(ax=ax1,x=data_numerical['age'],y=data_numerical['avg_glucose_level'],hue=data_categorical['stroke'],palette="OrRd")
ax1.set_title('Age and Average Glucose',fontweight='bold')
# BMI and Glucose
sns.scatterplot(ax=ax2,x=data_numerical['bmi'],y=data_numerical['avg_glucose_level'],hue=data_categorical['stroke'],palette="OrRd")
ax2.set_title('BMI and Average Glucose',fontweight='bold')
#removing spines
for i in ["top","right"]:
    ax0.spines[i].set_visible(False)
    ax1.spines[i].set_visible(False)
    ax2.spines[i].set_visible(False)

#@title
fig=plt.figure(figsize=(20,23))
background_color = '#f6f5f7'
fig.patch.set_facecolor(background_color) 
for indx,val in enumerate(data_categorical.columns):
    ax=plt.subplot(5,2,indx+1)
    ax.set_facecolor(background_color)
    ax.set_title(val,fontweight='bold',fontfamily='serif')
    for i in ['top','right']:
        ax.spines[i].set_visible(False)
    ax.grid(linestyle=':',axis='y')
    sns.countplot(data_categorical[val],palette='OrRd')

#@title
data_cat=data_train[['gender', 'hypertension', 'heart_disease', 'ever_married','work_type', 'Residence_type', 
                       'smoking_status', 'blood']]
fig=plt.figure(figsize=(20,23))
background_color = '#f6f5f7'
fig.patch.set_facecolor(background_color) 
for indx,val in enumerate(data_cat.columns):
    ax=plt.subplot(4,2,indx+1)
    ax.set_facecolor(background_color)
    ax.set_title(val,fontweight='bold',fontfamily='serif')
    for i in ['top','right']:
        ax.spines[i].set_visible(False)
    ax.grid(linestyle=':',axis='y')
    sns.countplot(data_cat[val],palette='OrRd_r',hue=data_train['stroke'])

"""# data preprocessing
## convert  data_categorical to data_numeric 
"""

label_gender = LabelEncoder()
label_married = LabelEncoder()
label_work = LabelEncoder()
label_residence = LabelEncoder()
label_smoking = LabelEncoder()
label_blood = LabelEncoder()

#@title
print('Unique gender: {}'.format(data_train['gender'].unique()))
print('Unique married: {}'.format(data_train['ever_married'].unique()))
print('Unique work type: {}'.format(data_train['work_type'].unique()))
print('Unique residence: {}'.format(data_train['Residence_type'].unique()))
print('Unique smoking_status: {}'.format(data_train['smoking_status'].unique()))
print('Unique blood: {}'.format(data_train['blood'].unique()))

data_train['gender'] = label_gender.fit_transform(data_train['gender'])
data_train['ever_married'] = label_married.fit_transform(data_train['ever_married'])
data_train['work_type']= label_work.fit_transform(data_train['work_type'])
data_train['Residence_type']= label_residence.fit_transform(data_train['Residence_type'])
data_train['smoking_status']= label_smoking.fit_transform(data_train['smoking_status'])
data_train['blood']= label_smoking.fit_transform(data_train['blood'])

data_train.head(13).T

"""#pre-processing data"""

from sklearn.model_selection import train_test_split
from collections import Counter
from sklearn.preprocessing import LabelEncoder, StandardScaler
from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN

X_raw = data_train.iloc[:,data_train.columns!='stroke']
y_raw = data_train.iloc[:,-1]

def over_sample_data(method):
  if method == 'SMOTE':
    over_sample = SMOTE(sampling_strategy = 'minority', kind = 'borderline2')
    X, y = over_sample.fit_sample(data_train.iloc[:,data_train.columns!='stroke'], data_train.iloc[:,-1])
  elif method == 'RandomOverSampler':
    over_sample = RandomOver(sampling_strategy = 'minority', kind = 'borderline2')
    X, y = over_sample.fit_sample(data_train.iloc[:,data_train.columns!='stroke'], data_train.iloc[:,-1])
  elif method == 'ADASYN':
    over_sample = ADASYN(sampling_strategy = 'minority')
    X, y = over_sample.fit_sample(data_train.iloc[:,data_train.columns!='stroke'], data_train.iloc[:,-1])
    
  return X, y

X, y = over_sample_data('SMOTE')
print('Resample dataset shape {}'.format(Counter(y_raw)))
print('Original dataset shape {}'.format(Counter(y)))

y

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0 ) 
x_train

def scaler(method):
  if method == 'StanderScaler':
    scaler = StandardScaler()
  elif method == 'MinMaxScaler':
    scaler = 'MinMaxScaler'
  scaled_X_train = scaler.fit_transform(x_train)
  scaled_X_test = scaler.transform(x_test)
  return scaled_X_train, scaled_X_test

std_X_train, std_X_test = scaler('StanderScaler')

"""# traning model"""

from sklearn import metrics
from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix

#using logistic
lr_model = LogisticRegression(solver='liblinear', penalty = 'l2', C = 0.01).fit(std_X_train, y_train)
lr_predicted = lr_model.predict(std_X_test)
predict_proba = lr_model.predict_proba(std_X_test)
model_train_score = lr_model.score(std_X_train, y_train)
model_score = lr_model.score(std_X_test, y_test)
f_score = f1_score(y_test, lr_predicted)
acc = accuracy_score(y_test, lr_predicted)
cm = confusion_matrix(y_test, lr_predicted)
print("Train_score: ", model_train_score)
print("Test_score: ", model_score)
print("f1_score: ", f_score)
plot_confusion_matrix(lr_model, std_X_test, y_test, cmap = "Blues" )
metrics.plot_roc_curve(lr_model, std_X_test, y_test)
f1_score, roc_auc_score, confusion_matrix

#using decision tree
dt_model = RandomForestClassifier(n_estimators=100,random_state=50).fit(std_X_train, y_train)
dt_predicted = dt_model.predict(std_X_test)
predict_proba = dt_model.predict_proba(std_X_test)
model_train_score = dt_model.score(std_X_train, y_train)
model_score = dt_model.score(std_X_test, y_test)
f_score = f1_score(y_test, dt_predicted)
acc = accuracy_score(y_test, dt_predicted)
cm = confusion_matrix(y_test, dt_predicted)
print("Train_score: ", model_train_score)
print("Test_score: ", model_score)
print("f1_score: ", f_score)
plot_confusion_matrix(dt_model, std_X_test, y_test, cmap = "Blues" )
metrics.plot_roc_curve(dt_model, std_X_test, y_test)
f1_score, roc_auc_score, confusion_matrix

#using svm
svm_model = SVC(kernel = 'linear', gamma = 0.1, C = 0.1, degree = 0.1).fit(std_X_train, y_train)
dt_predicted = svm_model.predict(std_X_test)
#predict_proba = svm_model.predict_proba(std_X_test)
model_train_score = svm_model.score(std_X_train, y_train)
model_score = svm_model.score(std_X_test, y_test)
f_score = f1_score(y_test, dt_predicted)
acc = accuracy_score(y_test, dt_predicted)
cm = confusion_matrix(y_test, dt_predicted)
print("Train_score: ", model_train_score)
print("Test_score: ", model_score)
print("f1_score: ", f_score)
plot_confusion_matrix(svm_model, std_X_test, y_test, cmap = "Blues" )
metrics.plot_roc_curve(svm_model, std_X_test, y_test)
f1_score, roc_auc_score, confusion_matrix

"""# model on data test

"""

data_test

"""## preprocessing data test"""

def fill_miss_value(data):
  num_of_miss_data = data.isnull().sum()
  missing_data = pd.concat([num_of_miss_data], axis=1, keys = ['Number of missing value'])
  return missing_data
fill_miss_value(data_test)

#fill data age
data_test['age'] = data_test['age'].interpolate(method = 'pad', limit = 2)
data_test['age'] = data_test['age'].fillna(int(np.mean(data_test['age'])))

#fill data hypertension
data_test['hypertension'] = data_test['hypertension'].interpolate(method = 'pad', limit = 3)

#fill heart disease
data_test['heart_disease'] = data_test['heart_disease'].interpolate(method = 'pad', limit = 4)

#fill data glucose
data_test['avg_glucose_level'] = data_test['avg_glucose_level'].interpolate(method = 'pad', limit = 2)
data_test['avg_glucose_level'] = data_test['avg_glucose_level'].fillna(int(np.mean(data_test['avg_glucose_level'])))

#fill data bmi
data_test['bmi'] = data_test['bmi'].interpolate(method = 'pad', limit = 2)
data_test['bmi'] = data_test['bmi'].fillna(int(np.mean(data_test['bmi'])))

#fill data height
data_test['height'] = data_test['height'].interpolate(method = 'pad', limit = 2)
data_test['height'] = data_test['height'].fillna(int(np.mean(data_test['height'])))

fill_miss_value(data_test)
# rows_with_nan = [index for index, row in data_test.iterrows() if row.isnull().any()]
# data_test['hypertension'][317]  = 0.0

data_test.describe()

id_test = data_test["id"].to_numpy()

data_test = data_test.drop(['id'], axis = 1)

data_test['gender'] = label_gender.fit_transform(data_test['gender'])
data_test['ever_married'] = label_married.fit_transform(data_test['ever_married'])
data_test['work_type']= label_work.fit_transform(data_test['work_type'])
data_test['Residence_type']= label_residence.fit_transform(data_test['Residence_type'])
data_test['smoking_status']= label_smoking.fit_transform(data_test['smoking_status'])
data_test['blood']= label_smoking.fit_transform(data_test['blood'])

data_test

std_test = StandardScaler().fit_transform(data_test)

y_public_test = lr_model.predict(std_test)
y_public_test
result = {
    "id": id_test,
    "stroke" :y_public_test_lr
}
result_submit = pd.DataFrame(result)
result_submit

result_submit.to_csv(r'/content/submission.csv', index = False)

"""#submit"""

# import math, requests

# def submit_prediction(df, sep=',', comment='', compression='gzip', **kwargs):
#     TOKEN='7f021785281f3c1430a0361389fb3884a76483f58d4551281109512ab337b5ad735daa0dd61d87ee99ea64158c299b5cede4f9684d16d88aae6accf9aea8f565'
#     URL='http://submission.mmlab.uit.edu.vn/api/submissions'
#     df.to_csv('temporary.dat', sep=sep, compression=compression, **kwargs)
#     r = requests.post(URL, headers={'Authorization': 'Bearer {}'.format(TOKEN)},files={'datafile': open('temporary.dat', 'rb')},data={'comment':comment, 'compression': compression})
#     if r.status_code == 429:
#         raise Exception('Submissions are too close. Next submission is only allowed in {} seconds.'.format(int(math.ceil(int(r.headers['x-rate-limit-remaining']) / 1000.0))))
#     if r.status_code != 200:
#         raise Exception(r.text)
        
# submit_prediction(result_submit, sep=',', index=True, comment='my submission')